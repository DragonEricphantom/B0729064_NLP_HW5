{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_HW05.ipynb","provenance":[],"collapsed_sections":[],"history_visible":true,"mount_file_id":"1n1cf2b8g6gKh5BfotHvU1OYySn-M9aLN","authorship_tag":"ABX9TyML0Pu+TU/IdAX+tErx5Cib"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7uYlBNvTX7DV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6d27c8b-d81f-4558-8d8c-787077cc5626"},"source":["import os\n","import json\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras import callbacks\n","import numpy as np\n","\n","batch_size = 16\n","epochs = 25\n","latent_dim = 256 # LSTM 的单元个数\n","num_samples = 1000 # 训练样本的大小\n","\n","en_characters = set()\n","ch_characters = set()\n","\n","# Opening JSON file\n","file = open('/content/drive/MyDrive/NatureLanguageProgram/training_data/translation2019zh_valid.json')\n","en_txt = []\n","ch_txt = []\n","txt = []\n","for line in file.readlines():\n","  data = json.loads(line)\n","  en_txt.append(data['english'])\n","  ch_txt.append(data['chinese'])\n","  txt.append(data)\n","\n","# for i in ch_txt:\n","#   print(i)\n","# for i in en_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   en.append(top)\n","# for i in ch_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   zh.append(top)\n","\n","for char in en_txt:\n","  if char not in en_characters:\n","    en_characters.add(char)\n","    # print(\"in char in en_txt \")\n","    # print(char)\n","for char in ch_txt:\n","  if char not in ch_characters:\n","    ch_characters.add(char)\n","    # print(\"in char in ch_txt \")\n","    # print(char)   \n","input_characters = sorted(list(en_characters))\n","target_characters = sorted(list(ch_characters))\n","num_encoder_tokens = len(en_characters)\n","num_decoder_tokens = len(ch_characters)\n","max_encoder_seq_length = max([ len(txt) for txt in en_txt])\n","max_decoder_seq_length = max([ len(txt) for txt in ch_txt])\n","\n","print('Nunmber of samples:', len(en_txt))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length of input:', max_encoder_seq_length)\n","print('Max sequence length of outputs:', max_decoder_seq_length)\n","\n","en_token_index = dict( [(char, i)for i, char in enumerate(input_characters)] )\n","ch_token_index = dict( [(char, i) for i, char in enumerate(target_characters)] )\n","\n","encoder_input_data = np.zeros((len(en_txt), max_encoder_seq_length, num_encoder_tokens), dtype=np.float32)\n","decoder_input_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","decoder_target_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","\n","for i, (en_txt, ch_txt) in enumerate(zip(en_txts, ch_txts)):\n","    # 对编码器的输入序列做one-hot\n","    for t, char in enumerate(en_txt):\n","        encoder_input_data[i, t, en_token_index[char]] = 1.0\n","    \n","    # 对解码器的输入与输出做序列做one-hot\n","    for t, char in enumerate(ch_txt):\n","        decoder_input_data[i, t, ch_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data 不包含开始字符，并且比decoder_input_data提前一步\n","            decoder_target_data[i, t-1, ch_token_index[char]] = 1.0\n","\n","# 定义编码器的输入\n","# encoder_inputs (None, num_encoder_tokens), None表示可以处理任意长度的序列\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","\n","# 编码器，要求其返回状态\n","encoder = LSTM(latent_dim, return_state=True)\n","\n","# 调用编码器，得到编码器的输出（输入其实不需要），以及状态信息 state_h 和 state_c\n","encoder_outpus, state_h, state_c = encoder(encoder_inputs)\n","\n","# 丢弃encoder_outputs, 我们只需要编码器的状态\n","encoder_state = [state_h, state_c]\n","\n","# 定义解码器的输入\n","# 同样的，None表示可以处理任意长度的序列\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","\n","# 接下来建立解码器，解码器将返回整个输出序列\n","# 并且返回其中间状态，中间状态在训练阶段不会用到，但是在推理阶段将是有用的\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# 将编码器输出的状态作为初始解码器的初始状态\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n","\n","# 添加全连接层\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# 定义整个模型\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# 定义回调函数\n","#callback_list = [callbacks.EarlyStopping(patience=10)]\n","# 编译模型\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","# 训练\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs = epochs,\n","          validation_split=0.2)\n","# 保存模型\n","model.save('s2s_2.h5')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Nunmber of samples: 39323\n","Number of unique input tokens: 39323\n","Number of unique output tokens: 39319\n","Max sequence length of input: 373\n","Max sequence length of outputs: 197\n"],"name":"stdout"}]}]}