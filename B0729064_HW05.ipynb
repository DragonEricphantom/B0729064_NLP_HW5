{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_HW05.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyCMJJxpRPxI","executionInfo":{"status":"ok","timestamp":1629392041509,"user_tz":-480,"elapsed":23748,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"83b18416-620e-40d4-9890-c32e73e78578"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7uYlBNvTX7DV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629396093392,"user_tz":-480,"elapsed":283817,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"704f8464-3e5c-45f8-adaa-a5a88e1a0dd9"},"source":["import os\n","import requests\n","import json\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras import callbacks\n","import numpy as np\n","\n","batch_size = 64\n","epochs = 100\n","latent_dim = 256 # LSTM 的单元个数\n","num_samples = 1000 # 训练样本的大小\n","\n","en_characters = set()\n","ch_characters = set()\n","\n","# Opening JSON file\n","file = open('/content/new2.json',encoding='utf-8')\n","#file = open('translation2019zh_valid.json', encoding='utf-8')\n","en_txt = []\n","ch_txt = []\n","txt = []\n","for line in file.readlines():\n","  data = json.loads(line)\n","  en_txt.append(data['english'])\n","  ch_txt.append(data['chinese'])\n","  txt.append(data)\n","\n","# for i in ch_txt:\n","#   print(i)\n","# for i in en_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   en.append(top)\n","# for i in ch_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   zh.append(top)\n","\n","for word in en_txt:\n","  # print(\"word = \")\n","  # print(word)\n","  for char in word:\n","    # print(\"char = \")\n","    # print(char)\n","    if char not in en_characters:\n","      en_characters.add(char)\n","\n","for word in ch_txt:\n","  for char in word:\n","    if char not in ch_characters:\n","      ch_characters.add(char)\n","    # print(\"in char in ch_txt \")\n","    # print(char)   \n","input_characters = sorted(list(en_characters))\n","target_characters = sorted(list(ch_characters))\n","num_encoder_tokens = len(en_characters)\n","num_decoder_tokens = len(ch_characters)\n","max_encoder_seq_length = max([ len(txt) for txt in en_txt])\n","max_decoder_seq_length = max([ len(txt) for txt in ch_txt])\n","\n","print('Nunmber of samples:', len(en_txt))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length of input:', max_encoder_seq_length)\n","print('Max sequence length of outputs:', max_decoder_seq_length)\n","\n","en_token_index = dict( [(char, i)for i, char in enumerate(input_characters)] )\n","ch_token_index = dict( [(char, i) for i, char in enumerate(target_characters)] )\n","\n","encoder_input_data = np.zeros((len(en_txt), max_encoder_seq_length, num_encoder_tokens), dtype=np.float32)\n","decoder_input_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","decoder_target_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","\n","for i, (en_txt, ch_txt) in enumerate(zip(en_txt, ch_txt)):\n","    # 对编码器的输入序列做one-hot\n","  for t, char in enumerate(en_txt):\n","    encoder_input_data[i,t,en_token_index[char]] = 1.0\n","    # 对解码器的输入与输出做序列做one-hot\n","  for t, char in enumerate(ch_txt):\n","    decoder_input_data[i,t,ch_token_index[char]] = 1.0\n","    if t > 0:\n","      # decoder_target_data 不包含开始字符，并且比decoder_input_data提前一步\n","      decoder_target_data[i, t-1, ch_token_index[char]] = 1.0\n","\n","# 定义编码器的输入\n","# encoder_inputs (None, num_encoder_tokens), None表示可以处理任意长度的序列\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","\n","# 编码器，要求其返回状态\n","encoder = LSTM(latent_dim, return_state=True)\n","\n","# 调用编码器，得到编码器的输出（输入其实不需要），以及状态信息 state_h 和 state_c\n","encoder_outpus, state_h, state_c = encoder(encoder_inputs)\n","\n","# 丢弃encoder_outputs, 我们只需要编码器的状态\n","encoder_state = [state_h, state_c]\n","\n","# 定义解码器的输入\n","# 同样的，None表示可以处理任意长度的序列\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","\n","# 接下来建立解码器，解码器将返回整个输出序列\n","# 并且返回其中间状态，中间状态在训练阶段不会用到，但是在推理阶段将是有用的\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# 将编码器输出的状态作为初始解码器的初始状态\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n","\n","# 添加全连接层\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# 定义整个模型\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# 定义回调函数\n","#callback_list = [callbacks.EarlyStopping(patience=10)]\n","# 编译模型\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","# 训练\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs = epochs,\n","          validation_split=0.2)\n","# 保存模型\n","model.save('s2s_2.h5')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Nunmber of samples: 1000\n","Number of unique input tokens: 113\n","Number of unique output tokens: 2400\n","Max sequence length of input: 218\n","Max sequence length of outputs: 92\n","Epoch 1/100\n","13/13 [==============================] - 8s 314ms/step - loss: 2.5528 - val_loss: 2.4443\n","Epoch 2/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4194 - val_loss: 2.4420\n","Epoch 3/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.4142 - val_loss: 2.4436\n","Epoch 4/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.4122 - val_loss: 2.4426\n","Epoch 5/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4110 - val_loss: 2.4406\n","Epoch 6/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4093 - val_loss: 2.4449\n","Epoch 7/100\n","13/13 [==============================] - 3s 205ms/step - loss: 2.4086 - val_loss: 2.4433\n","Epoch 8/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.4068 - val_loss: 2.4474\n","Epoch 9/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4068 - val_loss: 2.4482\n","Epoch 10/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.4059 - val_loss: 2.4467\n","Epoch 11/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4051 - val_loss: 2.4524\n","Epoch 12/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4052 - val_loss: 2.4495\n","Epoch 13/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4037 - val_loss: 2.4515\n","Epoch 14/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.4030 - val_loss: 2.4526\n","Epoch 15/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4039 - val_loss: 2.4504\n","Epoch 16/100\n","13/13 [==============================] - 3s 218ms/step - loss: 2.4026 - val_loss: 2.4519\n","Epoch 17/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4015 - val_loss: 2.4500\n","Epoch 18/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4004 - val_loss: 2.4522\n","Epoch 19/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3997 - val_loss: 2.4532\n","Epoch 20/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4000 - val_loss: 2.4528\n","Epoch 21/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3995 - val_loss: 2.4534\n","Epoch 22/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3989 - val_loss: 2.4545\n","Epoch 23/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3983 - val_loss: 2.4575\n","Epoch 24/100\n","13/13 [==============================] - 3s 216ms/step - loss: 2.3975 - val_loss: 2.4593\n","Epoch 25/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3972 - val_loss: 2.4584\n","Epoch 26/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3976 - val_loss: 2.4614\n","Epoch 27/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3972 - val_loss: 2.4630\n","Epoch 28/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3970 - val_loss: 2.4643\n","Epoch 29/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3963 - val_loss: 2.4671\n","Epoch 30/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3970 - val_loss: 2.4682\n","Epoch 31/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3960 - val_loss: 2.4654\n","Epoch 32/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3954 - val_loss: 2.4656\n","Epoch 33/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3937 - val_loss: 2.4730\n","Epoch 34/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3968 - val_loss: 2.4658\n","Epoch 35/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3952 - val_loss: 2.4717\n","Epoch 36/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3944 - val_loss: 2.4722\n","Epoch 37/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3942 - val_loss: 2.4733\n","Epoch 38/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3941 - val_loss: 2.4717\n","Epoch 39/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3939 - val_loss: 2.4725\n","Epoch 40/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3929 - val_loss: 2.4742\n","Epoch 41/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3934 - val_loss: 2.4744\n","Epoch 42/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3935 - val_loss: 2.4732\n","Epoch 43/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3919 - val_loss: 2.4733\n","Epoch 44/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3924 - val_loss: 2.4727\n","Epoch 45/100\n","13/13 [==============================] - 3s 216ms/step - loss: 2.3917 - val_loss: 2.4728\n","Epoch 46/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3910 - val_loss: 2.4733\n","Epoch 47/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3916 - val_loss: 2.4783\n","Epoch 48/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3910 - val_loss: 2.4735\n","Epoch 49/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3905 - val_loss: 2.4796\n","Epoch 50/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3905 - val_loss: 2.4817\n","Epoch 51/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3919 - val_loss: 2.4799\n","Epoch 52/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3911 - val_loss: 2.4796\n","Epoch 53/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3908 - val_loss: 2.4801\n","Epoch 54/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3902 - val_loss: 2.4803\n","Epoch 55/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3896 - val_loss: 2.4792\n","Epoch 56/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3889 - val_loss: 2.4841\n","Epoch 57/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3899 - val_loss: 2.4813\n","Epoch 58/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3893 - val_loss: 2.4835\n","Epoch 59/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3885 - val_loss: 2.4852\n","Epoch 60/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3881 - val_loss: 2.4840\n","Epoch 61/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3886 - val_loss: 2.4839\n","Epoch 62/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3885 - val_loss: 2.4829\n","Epoch 63/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3886 - val_loss: 2.4829\n","Epoch 64/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3880 - val_loss: 2.4838\n","Epoch 65/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3869 - val_loss: 2.4810\n","Epoch 66/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3880 - val_loss: 2.4850\n","Epoch 67/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3866 - val_loss: 2.4838\n","Epoch 68/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3863 - val_loss: 2.4840\n","Epoch 69/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3863 - val_loss: 2.4845\n","Epoch 70/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3859 - val_loss: 2.4803\n","Epoch 71/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3873 - val_loss: 2.4829\n","Epoch 72/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3849 - val_loss: 2.4849\n","Epoch 73/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3849 - val_loss: 2.4836\n","Epoch 74/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3843 - val_loss: 2.4861\n","Epoch 75/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3850 - val_loss: 2.4859\n","Epoch 76/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3842 - val_loss: 2.4824\n","Epoch 77/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3869 - val_loss: 2.4823\n","Epoch 78/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3836 - val_loss: 2.4868\n","Epoch 79/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3838 - val_loss: 2.4858\n","Epoch 80/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3829 - val_loss: 2.4840\n","Epoch 81/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3836 - val_loss: 2.4826\n","Epoch 82/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3843 - val_loss: 2.4840\n","Epoch 83/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3820 - val_loss: 2.4882\n","Epoch 84/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3813 - val_loss: 2.4812\n","Epoch 85/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3829 - val_loss: 2.4850\n","Epoch 86/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3813 - val_loss: 2.4864\n","Epoch 87/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3821 - val_loss: 2.4873\n","Epoch 88/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3805 - val_loss: 2.4849\n","Epoch 89/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3816 - val_loss: 2.4854\n","Epoch 90/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3799 - val_loss: 2.4808\n","Epoch 91/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3820 - val_loss: 2.4853\n","Epoch 92/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3793 - val_loss: 2.4843\n","Epoch 93/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3793 - val_loss: 2.4842\n","Epoch 94/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3791 - val_loss: 2.4859\n","Epoch 95/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3784 - val_loss: 2.4879\n","Epoch 96/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3782 - val_loss: 2.4879\n","Epoch 97/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3814 - val_loss: 2.4879\n","Epoch 98/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3794 - val_loss: 2.4879\n","Epoch 99/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3769 - val_loss: 2.4893\n","Epoch 100/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3778 - val_loss: 2.4830\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KsgmqOiIb99J"},"source":["from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0YVKMKH8lGmi","executionInfo":{"status":"error","timestamp":1629396999203,"user_tz":-480,"elapsed":280152,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"ceb4d846-852d-4771-8632-509ad0ca1d59"},"source":["import os\n","import requests\n","import json\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras import callbacks\n","import numpy as np\n","\n","batch_size = 64\n","epochs = 100\n","latent_dim = 256 # LSTM 的单元个数\n","num_samples = 1000 # 训练样本的大小\n","\n","en_characters = set()\n","ch_characters = set()\n","\n","# Opening JSON file\n","file = open('/content/new2.json',encoding='utf-8')\n","#file = open('translation2019zh_valid.json', encoding='utf-8')\n","en_txt = []\n","ch_txt = []\n","txt = []\n","for line in file.readlines():\n","  data = json.loads(line)\n","  en_txt.append(data['english'])\n","  ch_txt.append(data['chinese'])\n","  txt.append(data)\n","\n","# for i in ch_txt:\n","#   print(i)\n","# for i in en_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   en.append(top)\n","# for i in ch_txt:\n","#   top = jieba.analyse.extract_tags(i)\n","#   zh.append(top)\n","\n","for word in en_txt:\n","  # print(\"word = \")\n","  # print(word)\n","  for char in word:\n","    # print(\"char = \")\n","    # print(char)\n","    if char not in en_characters:\n","      en_characters.add(char)\n","\n","for word in ch_txt:\n","  for char in word:\n","    if char not in ch_characters:\n","      ch_characters.add(char)\n","    # print(\"in char in ch_txt \")\n","    # print(char)   \n","input_characters = sorted(list(en_characters))\n","target_characters = sorted(list(ch_characters))\n","num_encoder_tokens = len(en_characters)\n","num_decoder_tokens = len(ch_characters)\n","max_encoder_seq_length = max([ len(txt) for txt in en_txt])\n","max_decoder_seq_length = max([ len(txt) for txt in ch_txt])\n","\n","print('Nunmber of samples:', len(en_txt))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length of input:', max_encoder_seq_length)\n","print('Max sequence length of outputs:', max_decoder_seq_length)\n","\n","en_token_index = dict( [(char, i)for i, char in enumerate(input_characters)] )\n","ch_token_index = dict( [(char, i) for i, char in enumerate(target_characters)] )\n","\n","encoder_input_data = np.zeros((len(en_txt), max_encoder_seq_length, num_encoder_tokens), dtype=np.float32)\n","decoder_input_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","decoder_target_data = np.zeros((len(en_txt), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n","\n","for i, (en_txt, ch_txt) in enumerate(zip(en_txt, ch_txt)):\n","    # 对编码器的输入序列做one-hot\n","  for t, char in enumerate(en_txt):\n","    encoder_input_data[i,t,en_token_index[char]] = 1.0\n","    # 对解码器的输入与输出做序列做one-hot\n","  for t, char in enumerate(ch_txt):\n","    decoder_input_data[i,t,ch_token_index[char]] = 1.0\n","    if t > 0:\n","      # decoder_target_data 不包含开始字符，并且比decoder_input_data提前一步\n","      decoder_target_data[i, t-1, ch_token_index[char]] = 1.0\n","\n","# 定义编码器的输入\n","# encoder_inputs (None, num_encoder_tokens), None表示可以处理任意长度的序列\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","\n","# 编码器，要求其返回状态\n","encoder = LSTM(latent_dim, return_state=True)\n","\n","# 调用编码器，得到编码器的输出（输入其实不需要），以及状态信息 state_h 和 state_c\n","encoder_outpus, state_h, state_c = encoder(encoder_inputs)\n","\n","# 丢弃encoder_outputs, 我们只需要编码器的状态\n","encoder_state = [state_h, state_c]\n","\n","# 定义解码器的输入\n","# 同样的，None表示可以处理任意长度的序列\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","\n","# 接下来建立解码器，解码器将返回整个输出序列\n","# 并且返回其中间状态，中间状态在训练阶段不会用到，但是在推理阶段将是有用的\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","\n","# 将编码器输出的状态作为初始解码器的初始状态\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n","\n","# 添加全连接层\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# 定义整个模型\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# 定义回调函数\n","#callback_list = [callbacks.EarlyStopping(patience=10)]\n","# 编译模型\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","# 训练\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs = epochs,\n","          validation_split=0.2)\n","# 保存模型\n","model.save('s2s_2.h5')\n","\n","\n","\n","# 定义 sampling 模型\n","# 定义 encoder 模型，得到输出encoder_states\n","encoder_model = Model(encoder_inputs, encoder_state)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 得到解码器的输出以及中间状态\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs]+decoder_states)\n","\n","# 建立 数字->字符 的字典，用于恢复\n","reverse_input_char_index = dict([(i, char) for char, i in input_token_index.items()])\n","reverse_target_char_index = dict([(i, char) for char, i in target_token_index.items()])\n","\n","def decode_sequence(input_seq):\n","    # 将输入序列进行编码\n","    states_value = encoder_model.predict(input_seq)\n","    \n","    # 生成一个size=1的空序列\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # 将这个空序列的内容设置为开始字符\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","    \n","    # 进行字符恢复\n","    # 简单起见，假设batch_size = 1\n","    stop_condition = False\n","    decoded_sentence = ''\n","    \n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","        \n","        # sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","        \n","        # 退出条件：生成 \\n 或者 超过最大序列长度\n","        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length :\n","            stop_condition = True\n","            \n","        # 更新target_seq\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","        \n","        # 更新中间状态\n","        states_value = [h, c]\n","        \n","    return decoded_sentence\n","\n","for seq_index in range(1000, 1100):\n","    # batch_size = 1\n","    input_seq = encoder_input_data[seq_index:seq_index+1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    \n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Nunmber of samples: 1000\n","Number of unique input tokens: 113\n","Number of unique output tokens: 2400\n","Max sequence length of input: 218\n","Max sequence length of outputs: 92\n","Epoch 1/100\n","13/13 [==============================] - 7s 291ms/step - loss: 2.5593 - val_loss: 2.4474\n","Epoch 2/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.4198 - val_loss: 2.4415\n","Epoch 3/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.4137 - val_loss: 2.4411\n","Epoch 4/100\n","13/13 [==============================] - 3s 205ms/step - loss: 2.4104 - val_loss: 2.4402\n","Epoch 5/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4088 - val_loss: 2.4420\n","Epoch 6/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.4074 - val_loss: 2.4400\n","Epoch 7/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.4058 - val_loss: 2.4445\n","Epoch 8/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4066 - val_loss: 2.4467\n","Epoch 9/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4060 - val_loss: 2.4434\n","Epoch 10/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.4048 - val_loss: 2.4459\n","Epoch 11/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4038 - val_loss: 2.4471\n","Epoch 12/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.4034 - val_loss: 2.4478\n","Epoch 13/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.4038 - val_loss: 2.4509\n","Epoch 14/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.4034 - val_loss: 2.4465\n","Epoch 15/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.4023 - val_loss: 2.4511\n","Epoch 16/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.4020 - val_loss: 2.4530\n","Epoch 17/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.4013 - val_loss: 2.4505\n","Epoch 18/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.4004 - val_loss: 2.4537\n","Epoch 19/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4003 - val_loss: 2.4530\n","Epoch 20/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.4002 - val_loss: 2.4556\n","Epoch 21/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3992 - val_loss: 2.4607\n","Epoch 22/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3995 - val_loss: 2.4560\n","Epoch 23/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3980 - val_loss: 2.4565\n","Epoch 24/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3972 - val_loss: 2.4588\n","Epoch 25/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3979 - val_loss: 2.4596\n","Epoch 26/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3962 - val_loss: 2.4635\n","Epoch 27/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3965 - val_loss: 2.4614\n","Epoch 28/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3962 - val_loss: 2.4633\n","Epoch 29/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3960 - val_loss: 2.4665\n","Epoch 30/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3964 - val_loss: 2.4646\n","Epoch 31/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3951 - val_loss: 2.4686\n","Epoch 32/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3954 - val_loss: 2.4662\n","Epoch 33/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3947 - val_loss: 2.4648\n","Epoch 34/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3946 - val_loss: 2.4705\n","Epoch 35/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3947 - val_loss: 2.4687\n","Epoch 36/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3934 - val_loss: 2.4710\n","Epoch 37/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3931 - val_loss: 2.4707\n","Epoch 38/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3929 - val_loss: 2.4731\n","Epoch 39/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3923 - val_loss: 2.4694\n","Epoch 40/100\n","13/13 [==============================] - 3s 216ms/step - loss: 2.3922 - val_loss: 2.4710\n","Epoch 41/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3910 - val_loss: 2.4732\n","Epoch 42/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3924 - val_loss: 2.4725\n","Epoch 43/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3917 - val_loss: 2.4755\n","Epoch 44/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3913 - val_loss: 2.4749\n","Epoch 45/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3919 - val_loss: 2.4765\n","Epoch 46/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3913 - val_loss: 2.4789\n","Epoch 47/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3908 - val_loss: 2.4762\n","Epoch 48/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3908 - val_loss: 2.4772\n","Epoch 49/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3907 - val_loss: 2.4780\n","Epoch 50/100\n","13/13 [==============================] - 3s 206ms/step - loss: 2.3903 - val_loss: 2.4760\n","Epoch 51/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3893 - val_loss: 2.4791\n","Epoch 52/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3885 - val_loss: 2.4832\n","Epoch 53/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3895 - val_loss: 2.4771\n","Epoch 54/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3885 - val_loss: 2.4784\n","Epoch 55/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3885 - val_loss: 2.4769\n","Epoch 56/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3868 - val_loss: 2.4833\n","Epoch 57/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3871 - val_loss: 2.4808\n","Epoch 58/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3884 - val_loss: 2.4827\n","Epoch 59/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3868 - val_loss: 2.4804\n","Epoch 60/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3865 - val_loss: 2.4849\n","Epoch 61/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3874 - val_loss: 2.4835\n","Epoch 62/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3864 - val_loss: 2.4793\n","Epoch 63/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3864 - val_loss: 2.4852\n","Epoch 64/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3857 - val_loss: 2.4853\n","Epoch 65/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3855 - val_loss: 2.4808\n","Epoch 66/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3855 - val_loss: 2.4843\n","Epoch 67/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3844 - val_loss: 2.4851\n","Epoch 68/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3863 - val_loss: 2.4818\n","Epoch 69/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3854 - val_loss: 2.4800\n","Epoch 70/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3843 - val_loss: 2.4825\n","Epoch 71/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3840 - val_loss: 2.4822\n","Epoch 72/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3836 - val_loss: 2.4844\n","Epoch 73/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3832 - val_loss: 2.4864\n","Epoch 74/100\n","13/13 [==============================] - 3s 207ms/step - loss: 2.3830 - val_loss: 2.4871\n","Epoch 75/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3833 - val_loss: 2.4838\n","Epoch 76/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3833 - val_loss: 2.4819\n","Epoch 77/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3837 - val_loss: 2.4881\n","Epoch 78/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3819 - val_loss: 2.4827\n","Epoch 79/100\n","13/13 [==============================] - 3s 214ms/step - loss: 2.3814 - val_loss: 2.4841\n","Epoch 80/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3813 - val_loss: 2.4827\n","Epoch 81/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3821 - val_loss: 2.4841\n","Epoch 82/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3800 - val_loss: 2.4849\n","Epoch 83/100\n","13/13 [==============================] - 3s 211ms/step - loss: 2.3828 - val_loss: 2.4896\n","Epoch 84/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3809 - val_loss: 2.4818\n","Epoch 85/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3804 - val_loss: 2.4856\n","Epoch 86/100\n","13/13 [==============================] - 3s 215ms/step - loss: 2.3792 - val_loss: 2.4893\n","Epoch 87/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3794 - val_loss: 2.4827\n","Epoch 88/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3792 - val_loss: 2.4860\n","Epoch 89/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3785 - val_loss: 2.4898\n","Epoch 90/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3791 - val_loss: 2.4887\n","Epoch 91/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3778 - val_loss: 2.4860\n","Epoch 92/100\n","13/13 [==============================] - 3s 208ms/step - loss: 2.3763 - val_loss: 2.4856\n","Epoch 93/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3778 - val_loss: 2.4845\n","Epoch 94/100\n","13/13 [==============================] - 3s 212ms/step - loss: 2.3775 - val_loss: 2.4824\n","Epoch 95/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3746 - val_loss: 2.4833\n","Epoch 96/100\n","13/13 [==============================] - 3s 213ms/step - loss: 2.3786 - val_loss: 2.4848\n","Epoch 97/100\n","13/13 [==============================] - 3s 207ms/step - loss: 2.3751 - val_loss: 2.4865\n","Epoch 98/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3776 - val_loss: 2.4847\n","Epoch 99/100\n","13/13 [==============================] - 3s 210ms/step - loss: 2.3755 - val_loss: 2.4799\n","Epoch 100/100\n","13/13 [==============================] - 3s 209ms/step - loss: 2.3735 - val_loss: 2.4860\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2b6265f912f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# 建立 数字->字符 的字典，用于恢复\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mreverse_input_char_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_token_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0mreverse_target_char_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_token_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_token_index' is not defined"]}]}]}